{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a501025",
   "metadata": {},
   "source": [
    "# Phase II Evaluation Notebook\n",
    "\n",
    "This notebook documents the current status of the Marathi ASR + ST pipeline, demonstrates the preprocessing outputs, and captures the latest training/evaluation artefacts you can present during the Phase-II review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1fb001",
   "metadata": {},
   "source": [
    "## 1. Environment quick check\n",
    "Ensure you activate the project virtual environment (`source ml/bin/activate`) before running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f35ace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Current working directory: {Path.cwd()}\")\n",
    "print(f\"CUDA visible devices: {os.environ.get('CUDA_VISIBLE_DEVICES', 'not set')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57229f9b",
   "metadata": {},
   "source": [
    "## 2. Datasets\n",
    "The pipeline currently relies on two corpora:\n",
    "- **Common Voice 22.0 (Marathi)** for ASR pre-training.\n",
    "- **IWSLT 2023 (Marathiâ†’Hindi)** for speech translation fine-tuning.\n",
    "\n",
    "Both releases have already been preprocessed into 16kHz WAV clips with aligned manifests under `processed_data/`.\n",
    "If you need to regenerate the manifests, invoke the helper scripts in `scripts/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa36e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMON_VOICE_ROOT = Path('/home/anamika-rajesh/Desktop/ml/processed_data/common_voice_16khz')\n",
    "IWSLT_ROOT = Path('/home/anamika-rajesh/Desktop/ml/processed_data/iwslt_16khz')\n",
    "\n",
    "for name, root in [('Common Voice', COMMON_VOICE_ROOT), ('IWSLT', IWSLT_ROOT)]:\n",
    "    print(f'\\n{name} assets:')\n",
    "    print('  manifests:', sorted((root / 'manifests').glob('*.tsv')))\n",
    "    print('  sentencepiece model:', sorted((root / 'spm').glob('*.model')))\n",
    "    print('  dictionary:', (root / 'dict.txt').exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449bea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_manifest(manifest_path: Path, num_lines: int = 5):\n",
    "    if not manifest_path.exists():\n",
    "        raise FileNotFoundError(f'Manifest missing: {manifest_path}')\n",
    "    print(f'Previewing {manifest_path}:')\n",
    "    with manifest_path.open('r', encoding='utf-8') as handle:\n",
    "        for idx, line in enumerate(handle):\n",
    "            print(line.rstrip())\n",
    "            if idx + 1 >= num_lines:\n",
    "                break\n",
    "\n",
    "preview_manifest(COMMON_VOICE_ROOT / 'manifests' / 'train.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a5acdf",
   "metadata": {},
   "source": [
    "## 3. Preprocessing summary\n",
    "Scripts (`scripts/prepare_common_voice_dataset.py` and `scripts/prepare_iwslt_dataset.py`) cover resampling, manifest generation, and text normalization.\n",
    "\n",
    "Re-run them if the raw corpora are updated or if you need to regenerate artefacts with different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69544fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: regenerate Common Voice manifests (commented out to avoid accidental reprocessing)\n",
    "# !python scripts/prepare_common_voice_dataset.py \\ \n",
    "#     --input-root cv-corpus-22.0-2025-06-20-mr/cv-corpus-22.0-2025-06-20/mr \\ \n",
    "#     --output-root processed_data/common_voice_16khz \\ \n",
    "#     --num-workers 8\n",
    "\n",
    "# Example: regenerate IWSLT manifests\n",
    "# !python scripts/prepare_iwslt_dataset.py \\ \n",
    "#     --input-root datasets/iwslt2023_mr-hi \\ \n",
    "#     --output-root processed_data/iwslt_16khz \\ \n",
    "#     --num-workers 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e261502",
   "metadata": {},
   "source": [
    "## 4. Training configuration\n",
    "Fairseq Hydra configs live in `configs/fairseq/`.\n",
    "The current ASR pretraining run relies on `asr_pretrain.yaml`, while `st_finetune.yaml` consumes the ASR checkpoint for translation fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e31f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASR_CONFIG = Path('/home/anamika-rajesh/Desktop/ml/configs/fairseq/asr_pretrain.yaml')\n",
    "ST_CONFIG = Path('/home/anamika-rajesh/Desktop/ml/configs/fairseq/st_finetune.yaml')\n",
    "\n",
    "print(ASR_CONFIG.read_text()[:800])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee35890",
   "metadata": {},
   "source": [
    "## 5. Launch commands\n",
    "Training scripts thinly wrap `fairseq-hydra-train`.\n",
    "Execute them from the repository root with the `ml` virtual environment activated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90515c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ASR pretrain command:')\n",
    "print('  source ml/bin/activate && ./scripts/run_asr_pretrain.sh')\n",
    "print('ST fine-tune command:')\n",
    "print('  source ml/bin/activate && ./scripts/run_st_finetune.sh \\\\')\n",
    "print('        --encoder-checkpoint checkpoints/asr_pretrain/checkpoint_best.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ccfd95",
   "metadata": {},
   "source": [
    "## 6. Tracking metrics\n",
    "Once a training job completes, Fairseq logs appear in `checkpoints/**/train.log`. The snippet below extracts the last few updates for quick inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dab3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "LOG_PATH = Path('/home/anamika-rajesh/Desktop/ml/checkpoints/asr_pretrain/train.log')\n",
    "if LOG_PATH.exists():\n",
    "    print('Latest log entries:')\n",
    "    with LOG_PATH.open('r', encoding='utf-8') as handle:\n",
    "        for line in handle.readlines()[-10:]:\n",
    "            print(line.rstrip())\n",
    "else:\n",
    "    print('Train log not found yet. Run a training job to generate it.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fcc6c5",
   "metadata": {},
   "source": [
    "## 7. Current findings\n",
    "Summarise early WER/BLEU numbers, qualitative observations, and open questions here before the evaluation meeting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b151a6cb",
   "metadata": {},
   "source": [
    "## 8. Next actions\n",
    "- [ ] Finish the ongoing ASR pre-training run and archive the best checkpoint.\n",
    "- [ ] Kick off ST fine-tuning with the updated encoder.\n",
    "- [ ] Run inference on the dev/test sets and record WER/BLEU.\n",
    "- [ ] Update this notebook with final metrics, plots, and analysis."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
