{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4542170f-c202-4329-bc5e-c18b312fa9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp312-cp312-win_amd64.whl.metadata (10 kB)\n",
      "Downloading sentencepiece-0.2.1-cp312-cp312-win_amd64.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.1/1.1 MB 16.9 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5cb6e8b4-d5f7-4b67-8ad3-a0b34b0386a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "import sentencepiece as spm\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0fb1915-495f-43e5-a764-70d502904dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path(\"C:/Users/anami/Desktop/ML\")\n",
    "PROCESSED_DATA_ROOT = Path(\"C:/Users/anami/Desktop/ML/processed_data\")\n",
    "SAMPLE_RATE = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eaf8a0f4-3205-4c45-aeeb-3677e70cdcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASR_DATA_ROOT = PROCESSED_DATA_ROOT / \"common_voice_16khz\"\n",
    "CONFIG_DIR = PROJECT_ROOT / \"fairseq\" / \"configs\"\n",
    "MANIFEST_DIR = ASR_DATA_ROOT / \"manifests\"\n",
    "SPM_DIR = ASR_DATA_ROOT / \"spm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab8b4d30-a6e8-45fc-b012-53058bac303a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV_MANIFEST = PROCESSED_DATA_ROOT / \"dev_manifest.jsonl\"\n",
    "MANIFEST_SOURCES: Dict[str, Path] = {\n",
    "    \"train\": PROCESSED_DATA_ROOT / \"train_manifest.jsonl\",\n",
    "    \"dev\": DEV_MANIFEST if DEV_MANIFEST.exists() else PROCESSED_DATA_ROOT / \"validation_manifest.jsonl\",\n",
    "    \"test\": PROCESSED_DATA_ROOT / \"test_manifest.jsonl\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2d99787-31fe-4968-9023-b881df6204ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_manifest_entries(manifest_path: Path) -> List[dict]:\n",
    "    \"\"\"Loads entries from a JSONL manifest.\"\"\"\n",
    "    entries: List[dict] = []\n",
    "    with manifest_path.open(\"r\", encoding=\"utf-8\") as f_in:\n",
    "        for line_no, line in enumerate(f_in, start=1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                entries.append(json.loads(line))\n",
    "            except json.JSONDecodeError as exc:\n",
    "                print(f\"[WARNING] Skipping malformed line {line_no} in {manifest_path.name}: {exc}\")\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1857d84f-2ca8-4e2c-af31-187674e415d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_directories():\n",
    "    print(\"--- 1. Setting up directories and paths ---\")\n",
    "    for path in (CONFIG_DIR, MANIFEST_DIR, SPM_DIR):\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"Project Root: {PROJECT_ROOT}\")\n",
    "    print(f\"Processed Data Root: {PROCESSED_DATA_ROOT}\")\n",
    "    print(f\"ASR Data (Common Voice) Root: {ASR_DATA_ROOT}\")\n",
    "    print(f\"Manifest Directory: {MANIFEST_DIR}\")\n",
    "    print(f\"SentencePiece Directory: {SPM_DIR}\")\n",
    "    print(f\"Configuration Directory: {CONFIG_DIR}\\n\")\n",
    "\n",
    "    if not ASR_DATA_ROOT.exists():\n",
    "        print(f\"[ERROR] ASR data directory not found at: {ASR_DATA_ROOT}\")\n",
    "        print(\"Please double-check the 'PROCESSED_DATA_ROOT' path in the script.\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c147b99e-2017-481b-b139-4566879e5811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_audio_path(audio_filepath: str) -> Path:\n",
    "    audio_path = Path(audio_filepath)\n",
    "    if not audio_path.is_absolute():\n",
    "        audio_path = (PROCESSED_DATA_ROOT / audio_path).resolve(strict=False)\n",
    "    else:\n",
    "        audio_path = audio_path.resolve(strict=False)\n",
    "\n",
    "    try:\n",
    "        audio_rel = audio_path.relative_to(ASR_DATA_ROOT.resolve(strict=False))\n",
    "    except ValueError as exc:\n",
    "        raise ValueError(f\"Audio file {audio_path} is not inside {ASR_DATA_ROOT}\") from exc\n",
    "    return audio_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "481092d4-9f84-4475-a683-8b3c7cce47a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_fairseq_manifests() -> List[str]:\n",
    "    \"\"\"Converts JSONL manifests into Fairseq TSV manifests and returns train transcripts.\"\"\"\n",
    "    print(\"--- 2. Building Fairseq TSV manifests ---\")\n",
    "    train_transcripts: List[str] = []\n",
    "\n",
    "    for split, manifest_path in MANIFEST_SOURCES.items():\n",
    "        if not manifest_path.exists():\n",
    "            print(f\"[ERROR] Required manifest not found: {manifest_path}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        entries = load_manifest_entries(manifest_path)\n",
    "        if not entries:\n",
    "            print(f\"[ERROR] Manifest {manifest_path} is empty.\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        rows = []\n",
    "        skipped = 0\n",
    "        for entry in entries:\n",
    "            text = entry.get(\"text\")\n",
    "            audio_fp = entry.get(\"audio_filepath\")\n",
    "            if not text or not audio_fp:\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                audio_rel = resolve_audio_path(audio_fp)\n",
    "            except ValueError as exc:\n",
    "                print(f\"[WARNING] {exc}\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            n_frames = entry.get(\"num_frames\") or entry.get(\"n_frames\")\n",
    "            if n_frames is None:\n",
    "                duration = entry.get(\"duration\")\n",
    "                if duration is None:\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "                n_frames = int(round(duration * SAMPLE_RATE))\n",
    "\n",
    "            speaker = entry.get(\"speaker\") or entry.get(\"client_id\") or \"unknown\"\n",
    "            utt_id = entry.get(\"id\") or audio_rel.stem\n",
    "            rows.append((utt_id, audio_rel.as_posix(), n_frames, text, speaker))\n",
    "\n",
    "            if split == \"train\":\n",
    "                train_transcripts.append(text)\n",
    "\n",
    "        if not rows:\n",
    "            print(f\"[ERROR] No usable rows found in {manifest_path}.\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        output_path = MANIFEST_DIR / f\"{split}.tsv\"\n",
    "        with output_path.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "            f_out.write(\"id\\taudio\\tn_frames\\ttgt_text\\tspeaker\\n\")\n",
    "            for utt_id, audio_rel, n_frames, text, speaker in rows:\n",
    "                f_out.write(f\"{utt_id}\\t{audio_rel}\\t{n_frames}\\t{text}\\t{speaker}\\n\")\n",
    "\n",
    "        print(f\"[OK] Wrote {len(rows)} rows to {output_path} ({skipped} skipped).\")\n",
    "\n",
    "    if not train_transcripts:\n",
    "        print(\"[ERROR] No transcripts collected from the training manifest.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    print()\n",
    "    return train_transcripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ba233732-4d6e-42d8-a933-ddd57c77a97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tokenizer_and_build_dict(transcripts: List[str], model_prefix: Path, dict_path: Path, vocab_size: int):\n",
    "    print(\"--- 3. Generating Tokenizer and Dictionary ---\")\n",
    "    model_prefix.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(\"w\", encoding=\"utf-8\", delete=False) as tmp:\n",
    "        for line in transcripts:\n",
    "            tmp.write(f\"{line}\\n\")\n",
    "        tmp_path = Path(tmp.name)\n",
    "\n",
    "    spm_command = (\n",
    "        f\"--input={tmp_path} --model_prefix={model_prefix} \"\n",
    "        f\"--vocab_size={vocab_size} --character_coverage=1.0 \"\n",
    "        f\"--model_type=unigram --pad_id=3 --pad_piece=<pad> \"\n",
    "        f\"--unk_id=0 --bos_id=1 --eos_id=2\"\n",
    "    )\n",
    "    spm.SentencePieceTrainer.train(spm_command)\n",
    "    model_file = model_prefix.with_suffix(\".model\")\n",
    "    tmp_path.unlink(missing_ok=True)\n",
    "    print(f\"SentencePiece model saved to: {model_file}\")\n",
    "\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(str(model_file))\n",
    "\n",
    "    with dict_path.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "        for i in range(sp.get_piece_size()):\n",
    "            f_out.write(f\"{sp.id_to_piece(i)} 1\\n\")\n",
    "    print(f\"Fairseq dictionary saved to: {dict_path}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3769c4ed-17f0-4f4f-8a55-76e9b60252dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fairseq_configs():\n",
    "    \"\"\"Generates the main training and dataset-specific YAML files for Fairseq.\"\"\"\n",
    "    print(\"--- 4. Generating Fairseq YAML Configuration Files ---\")\n",
    "\n",
    "    asr_pretrain_config = {\n",
    "        \"defaults\": [\"_self_\"],\n",
    "        \"common\": {\"fp16\": True, \"log_format\": \"json\", \"log_interval\": 100, \"seed\": 1337},\n",
    "        \"task\": {\n",
    "            \"_name\": \"speech_to_text\",\n",
    "            \"data\": str(MANIFEST_DIR),\n",
    "            \"config_yaml\": \"config_asr.yaml\",\n",
    "            \"max_source_positions\": 3000,\n",
    "            \"max_target_positions\": 1024,\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"_name\": \"s2t_conformer\",\n",
    "            \"input_feat_per_channel\": 80,\n",
    "            \"encoder_layers\": 12,\n",
    "            \"encoder_embed_dim\": 512,\n",
    "            \"encoder_ffn_embed_dim\": 2048,\n",
    "            \"encoder_attention_heads\": 8,\n",
    "            \"decoder_layers\": 6,\n",
    "            \"decoder_embed_dim\": 512,\n",
    "            \"decoder_ffn_embed_dim\": 2048,\n",
    "            \"decoder_attention_heads\": 8,\n",
    "            \"dropout\": 0.15,\n",
    "        },\n",
    "        \"criterion\": {\"_name\": \"label_smoothed_cross_entropy\", \"label_smoothing\": 0.1, \"report_accuracy\": True},\n",
    "        \"optimizer\": {\"_name\": \"adam\", \"adam_betas\": \"(0.9, 0.98)\", \"lr\": [0.002]},\n",
    "        \"lr_scheduler\": {\"_name\": \"inverse_sqrt\", \"warmup_updates\": 10000},\n",
    "        \"checkpoint\": {\n",
    "            \"save_dir\": str(PROJECT_ROOT / \"fairseq\" / \"checkpoints\" / \"asr_pretrain\"),\n",
    "            \"best_checkpoint_metric\": \"wer\",\n",
    "            \"patience\": 10,\n",
    "            \"save_interval_updates\": 500,\n",
    "            \"keep_interval_updates\": 5,\n",
    "        },\n",
    "        \"dataset\": {\n",
    "            \"num_workers\": 4,\n",
    "            \"max_tokens\": 32000,\n",
    "            \"batch_size\": None,\n",
    "            \"valid_subset\": \"dev\",\n",
    "            \"skip_invalid_size_inputs_valid_test\": True,\n",
    "        },\n",
    "        \"eval\": {\"eval_wer\": True, \"eval_wer_config\": {\"beam\": 5, \"max_len_a\": 0, \"max_len_b\": 200}},\n",
    "    }\n",
    "    main_config_path = CONFIG_DIR / \"asr_pretrain.yaml\"\n",
    "    with main_config_path.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "        yaml.dump(asr_pretrain_config, f_out, sort_keys=False, indent=2)\n",
    "    print(f\"Main training config written to: {main_config_path}\")\n",
    "\n",
    "    dataset_config = {\n",
    "        \"sample_rate\": SAMPLE_RATE,\n",
    "        \"input_channels\": 1,\n",
    "        \"input_feat_per_channel\": 80,\n",
    "        \"use_audio_input\": True,\n",
    "        \"standardize_audio\": False,\n",
    "        \"audio_root\": \"\",\n",
    "        \"vocab_filename\": \"dict.txt\",\n",
    "        \"bpe_tokenizer\": {\n",
    "            \"bpe\": \"sentencepiece\",\n",
    "            \"sentencepiece_model\": str(Path(\"..\") / \"spm\" / \"mr_asr.model\"),\n",
    "        },\n",
    "        \"transforms\": {\n",
    "            \"_train\": [\"utterance_cmvn\", \"specaugment\"],\n",
    "            \"valid\": [\"utterance_cmvn\"],\n",
    "            \"test\": [\"utterance_cmvn\"],\n",
    "        },\n",
    "        \"specaugment\": {\n",
    "            \"time_wrap_W\": 0,\n",
    "            \"freq_mask_N\": 1,\n",
    "            \"freq_mask_F\": 27,\n",
    "            \"time_mask_N\": 1,\n",
    "            \"time_mask_T\": 100,\n",
    "            \"time_mask_p\": 1.0,\n",
    "        },\n",
    "    }\n",
    "    dataset_config_path = MANIFEST_DIR / \"config_asr.yaml\"\n",
    "    with dataset_config_path.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "        yaml.dump(dataset_config, f_out, sort_keys=False, indent=2)\n",
    "    print(f\"Dataset config written to: {dataset_config_path}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "431f9e93-c830-4856-905e-1f478d72a28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Setting up directories and paths ---\n",
      "Project Root: C:\\Users\\anami\\Desktop\\ML\n",
      "Processed Data Root: C:\\Users\\anami\\Desktop\\ML\\processed_data\n",
      "ASR Data (Common Voice) Root: C:\\Users\\anami\\Desktop\\ML\\processed_data\\common_voice_16khz\n",
      "Manifest Directory: C:\\Users\\anami\\Desktop\\ML\\processed_data\\common_voice_16khz\\manifests\n",
      "SentencePiece Directory: C:\\Users\\anami\\Desktop\\ML\\processed_data\\common_voice_16khz\\spm\n",
      "Configuration Directory: C:\\Users\\anami\\Desktop\\ML\\fairseq\\configs\n",
      "\n",
      "Successfully loaded 1330 training transcripts.\n",
      "--- 3. Generating Tokenizer and Dictionary ---\n",
      "SentencePiece model saved to: C:\\Users\\anami\\Desktop\\ML\\processed_data\\common_voice_16khz\\spm\\mr_asr.model\n",
      "Fairseq dictionary saved to: C:\\Users\\anami\\Desktop\\ML\\processed_data\\common_voice_16khz\\dict.txt\n",
      "\n",
      "--- 4. Generating Fairseq YAML Configuration Files ---\n",
      "Main training config written to: C:\\Users\\anami\\Desktop\\ML\\fairseq\\configs\\asr_pretrain.yaml\n",
      "Dataset config written to: C:\\Users\\anami\\Desktop\\ML\\processed_data\\common_voice_16khz\\manifests\\config_asr.yaml\n",
      "\n",
      "--- 4. Ready for Training ---\n",
      "All configuration files have been generated successfully.\n",
      "\n",
      "To launch the training job, run the following command in your terminal:\n",
      " (Ensure your virtual environment is active and you are in the project root directory)\n",
      "\n",
      "######################################################################\n",
      "fairseq-hydra-train \\\n",
      "  --config-dir C:/Users/anami/Desktop/ML/fairseq/configs \\\n",
      "  --config-name asr_pretrain\n",
      "######################################################################\n",
      "\n",
      "To perform a quick test of your configuration, add the --dry-run flag.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function to run the setup workflow.\"\"\"\n",
    "    setup_directories()\n",
    "\n",
    "    # Load transcripts from the manifest file\n",
    "    train_manifest_path = ASR_DATA_ROOT / \"manifests\" / \"train.tsv\"\n",
    "    try:\n",
    "        df = pd.read_csv(train_manifest_path, sep=\"\\t\", header=None, skiprows=1)\n",
    "        df.columns = ['id', 'audio', 'n_frames', 'tgt_text', 'speaker']\n",
    "        marathi_transcripts = df[\"tgt_text\"].dropna().tolist()\n",
    "        print(f\"Successfully loaded {len(marathi_transcripts)} training transcripts.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Could not process manifest file at {train_manifest_path}: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Generate tokenizer and dictionary\n",
    "    train_tokenizer_and_build_dict(\n",
    "        transcripts=marathi_transcripts,\n",
    "        model_prefix=ASR_DATA_ROOT / \"spm\" / \"mr_asr\",\n",
    "        dict_path=ASR_DATA_ROOT / \"dict.txt\",\n",
    "        vocab_size=4000\n",
    "    )\n",
    "\n",
    "    # Generate Fairseq configuration files\n",
    "    generate_fairseq_configs()\n",
    "\n",
    "    # Print the final command for the user\n",
    "    training_command = (\n",
    "        f\"fairseq-hydra-train \\\\\\n\"\n",
    "        f\"  --config-dir {CONFIG_DIR.as_posix()} \\\\\\n\"\n",
    "        f\"  --config-name asr_pretrain\"\n",
    "    )\n",
    "    \n",
    "    print(\"--- 4. Ready for Training ---\")\n",
    "    print(\"All configuration files have been generated successfully.\")\n",
    "    print(\"\\nTo launch the training job, run the following command in your terminal:\")\n",
    "    print(\" (Ensure your virtual environment is active and you are in the project root directory)\")\n",
    "    print(\"\\n\" + \"#\" * 70)\n",
    "    print(training_command)\n",
    "    print(\"#\" * 70)\n",
    "    print(\"\\nTo perform a quick test of your configuration, add the --dry-run flag.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # To run this script, open a terminal, navigate to your project directory \n",
    "    # (C:/Users/anami/Desktop/ML), activate your environment, and then run:\n",
    "    # python run_pretraining_setup.py\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744c6ef7-b36e-4273-843c-90c2866630bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
